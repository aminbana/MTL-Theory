{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from random import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# implement set seed function for reproducibility including torch, np and random\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def random_unit_vector(p):\n",
    "    vec = torch.randn(p)\n",
    "    vec_norm = torch.linalg.norm(vec)\n",
    "    return vec / vec_norm\n",
    "\n",
    "def get_vectors(T, p, theta, norms):\n",
    "    # Function to create a random vector of size p with a norm of 1\n",
    "\n",
    "\n",
    "    # Initialize the list of vectors with the first unit vector\n",
    "    vectors = [random_unit_vector(p)]\n",
    "\n",
    "    for _ in range(1, T):\n",
    "        new_vec = random_unit_vector(p)\n",
    "\n",
    "        # Adjust the new vector to make an angle theta with the previous vectors\n",
    "        # This part is simplified and may not always ensure the exact theta angle in higher dimensions\n",
    "        for v in vectors:\n",
    "            cos_theta = math.cos(theta)\n",
    "            projection = torch.dot(v, new_vec) * v\n",
    "            new_vec = cos_theta * v + math.sqrt(1 - cos_theta ** 2) * (new_vec - projection)\n",
    "            new_vec = new_vec / torch.linalg.norm(new_vec)\n",
    "\n",
    "        vectors.append(new_vec)\n",
    "\n",
    "    vectors = torch.stack(vectors)\n",
    "    return vectors * norms\n",
    "\n",
    "def get_true_tasks(T, s, u, theta, norms, norm2 = 1):\n",
    "    vectors = get_vectors(T, s, theta, norms)\n",
    "    tasks = []\n",
    "    for i in range(T):\n",
    "        v = torch.zeros(u*T)\n",
    "        v[i*u:(i+1)*u] = random_unit_vector(u) * norm2\n",
    "        \n",
    "        v = torch.cat([vectors[i], v], dim=0)\n",
    "        \n",
    "        tasks.append(v)\n",
    "        \n",
    "    return tasks\n",
    "\n",
    "def get_tasks(T,p,true_tasks,theta,norms, selection_method = 'random'):\n",
    "    p_ = len (true_tasks[0])\n",
    "    \n",
    "    if selection_method == 'random':\n",
    "        return get_vectors(T, p, theta, norms)\n",
    "    \n",
    "    if p_ < p:\n",
    "        expanded_tasks = []\n",
    "        for v in true_tasks:\n",
    "            v_ = torch.zeros(p)\n",
    "            v_[:p_] = v\n",
    "            expanded_tasks.append(v_)\n",
    "        return expanded_tasks\n",
    "    else:\n",
    "        if selection_method == 'crop':\n",
    "            return [v[:p] for v in true_tasks]\n",
    "        elif selection_method == 'random_crop':\n",
    "            idx = sorted ([i.item() for i in torch.randperm(p_)[:p]])\n",
    "            return [v[idx] for v in true_tasks]\n",
    "        else:\n",
    "            raise ValueError('invalid selection method')\n",
    "\n",
    "\n",
    "def get_X_and_Y(n, p, T, sigma, tasks):\n",
    "    # add additive gaussian noise to the labels\n",
    "    X = [torch.randn(p, n) for _ in range(T)]\n",
    "    Y = [task @ X[i] + sigma * torch.randn(n) for i, task in enumerate(tasks)]\n",
    "    return X, Y\n",
    "\n",
    "def get_loss(w,w_star):\n",
    "    return torch.linalg.norm(w-w_star)**2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def solve_eq(X,y,lambda_,w0=None):\n",
    "    \n",
    "    p,n = X.shape\n",
    "    \n",
    "    if w0 is None:\n",
    "        w0 = torch.zeros(p)\n",
    "    \n",
    "    if p >= n + 1:\n",
    "        w = w0 + X @ (X.T @ X).inverse() @ (y - X.T @ w0)\n",
    "    else:\n",
    "        try:\n",
    "            w = (X @ X.T + lambda_ * torch.eye(p)).inverse() @ (X @ y + lambda_ * w0)\n",
    "        except:\n",
    "            print (X.shape, y.shape, w0.shape)\n",
    "            raise ValueError('invalid shape')\n",
    "    return w\n",
    "\n",
    "def solve_multi_task(n, p, T, l, X_1T, Y_1T):\n",
    "\n",
    "    X_1T = torch.cat(X_1T, -1)\n",
    "    Y_1T = torch.cat(Y_1T, -1)\n",
    "    \n",
    "    w = solve_eq(X_1T,Y_1T,l)\n",
    "    \n",
    "    return w\n",
    "\n",
    "def evaluate_multi_task(n , p, T, l, sigma, tasks):\n",
    "    X_1T, Y_1T = get_X_and_Y(n, p, T, sigma, tasks)\n",
    "    w = solve_multi_task(n, p, T, l, X_1T, Y_1T)\n",
    "\n",
    "    average_error = 0\n",
    "    for i in range(T):\n",
    "        w_star = tasks[i]\n",
    "        average_error += get_loss(w,w_star)\n",
    "    average_error /= T\n",
    "    return average_error\n",
    "\n",
    "def evaluate_multiple_multi_tasks(n, p, T, l, sigma, tasks, num_trials):\n",
    "    average_errors = []\n",
    "    for _ in range(num_trials):\n",
    "        average_errors.append(evaluate_multi_task(n, p, T, l, sigma, tasks))\n",
    "    return np.mean (average_errors)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def solve_single_task(n, p, T, l, X_1T, Y_1T):\n",
    "\n",
    "    X_1T = torch.cat(X_1T, -1)\n",
    "    Y_1T = torch.cat(Y_1T, -1)\n",
    "\n",
    "    ws = []\n",
    "    for i in range(T):\n",
    "        X = X_1T[:, i*n:(i+1)*n]\n",
    "        Y = Y_1T[i*n:(i+1)*n]\n",
    "        w = solve_eq(X,Y,l)\n",
    "        ws.append(w)\n",
    "\n",
    "    return ws\n",
    "\n",
    "def evaluate_single_task(n, p, T, l, sigma, tasks):\n",
    "    X_1T, Y_1T = get_X_and_Y(n, p, T, sigma, tasks)\n",
    "    ws = solve_single_task(n, p, T, l, X_1T, Y_1T)\n",
    "\n",
    "    average_error = 0\n",
    "    for i in range(T):\n",
    "        w_star = tasks[i]\n",
    "        w = ws[i]\n",
    "        average_error += get_loss(w,w_star)\n",
    "    average_error /= T\n",
    "    return average_error\n",
    "\n",
    "def evaluate_multiple_single_tasks(n, p, T, l, sigma, tasks, num_trials):\n",
    "    average_errors = []\n",
    "    for _ in range(num_trials):\n",
    "        average_errors.append(evaluate_single_task(n, p, T, l, sigma, tasks))\n",
    "    return np.mean (average_errors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def solve_sequential(n, p, T, l, X_1T, Y_1T):\n",
    "\n",
    "    ws = []\n",
    "\n",
    "    for i in range(T):\n",
    "        X = X_1T[i]\n",
    "        Y = Y_1T[i]\n",
    "        w = solve_eq(X,Y,l)\n",
    "        ws.append(w)\n",
    "    return ws\n",
    "\n",
    "def solve_continual(n, p, T, l, X_1T, Y_1T):\n",
    "    ws = []\n",
    "    w0 = torch.zeros(p)\n",
    "\n",
    "\n",
    "    for i in range(T):\n",
    "        X = X_1T[i]\n",
    "        Y = Y_1T[i]\n",
    "        \n",
    "        w = solve_eq(X,Y,l,w0)\n",
    "        \n",
    "        w0 = w\n",
    "        ws.append(w)\n",
    "    return ws\n",
    "\n",
    "def solve_mem(n, k, p, T, l, X_1T, Y_1T):\n",
    "\n",
    "    ws = []\n",
    "    w0 = torch.zeros(p)\n",
    "    \n",
    "    mem_X = []\n",
    "    mem_Y = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        X_t = X_1T[t]\n",
    "        Y_t = Y_1T[t]\n",
    "        \n",
    "        \n",
    "        X = torch.cat(mem_X + [X_t], dim=1)\n",
    "        Y = torch.cat(mem_Y + [Y_t], dim=0)\n",
    "        \n",
    "        w = solve_eq(X,Y,l,w0)\n",
    "        \n",
    "        mem_X.append(X_t[:, n-k:])\n",
    "        mem_Y.append(Y_t[n-k:])\n",
    "\n",
    "        ws.append(w)\n",
    "\n",
    "    return ws\n",
    "\n",
    "def solve_memreg(n, k, p, T, l, X_1T, Y_1T):\n",
    "\n",
    "    ws = []\n",
    "    w0 = torch.zeros(p)\n",
    "    \n",
    "    mem_X = []\n",
    "    mem_Y = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        X_t = X_1T[t]\n",
    "        Y_t = Y_1T[t]\n",
    "        \n",
    "        X = torch.cat(mem_X + [X_t], dim=1)\n",
    "        Y = torch.cat(mem_Y + [Y_t], dim=0)\n",
    "        \n",
    "        w = solve_eq(X,Y,l,w0)\n",
    "        \n",
    "        mem_X.append(X_t[:, n-k:])\n",
    "        mem_Y.append(Y_t[n-k:])\n",
    "\n",
    "        ws.append(w)\n",
    "        w0 = w\n",
    "\n",
    "    return ws\n",
    "\n",
    "def predict_theory_continual(n, p, T, l, sigma, tasks):\n",
    "    if n + 1 <= p:\n",
    "        r = 1 - n/p\n",
    "        G1 = r**T/T * sum([w.norm()**2 for w in tasks])\n",
    "        G2 = 1/T * sum([n * r ** (T - (i+1)) / p * sum([(wk - wi).norm()**2 for wk in tasks]) for i, wi in enumerate(tasks)])\n",
    "        G3 = p * sigma**2 / (p - n - 1 + 1e-7) * (1 - r**T)\n",
    "        return G1 + G2 + G3\n",
    "\n",
    "    elif p + 1 <= n:\n",
    "        wT = tasks[-1]\n",
    "        G1 = 1/T * sum([(wT - w).norm()**2 for w in tasks])\n",
    "        G2 = p * sigma**2 / (n - p - 1 + 1e-7)\n",
    "        return G1 + G2\n",
    "\n",
    "def predict_theory_memreg(n, k, p, T, l, sigma, tasks, t = None):\n",
    "    if t is None:\n",
    "        t = T\n",
    "    # assume t to be indexed from 1, i.e. t = 1, 2, ..., T\n",
    "    \n",
    "    ns = [k] * (t-1) + [n]\n",
    "    ws = tasks\n",
    "    nb = sum(ns)\n",
    "    if nb + 1 <= p:\n",
    "\n",
    "        G1 = 1/T * sum ([\n",
    "            sum([\n",
    "                ns[s]/p * (ws[s] - ws[i]).norm()**2\n",
    "            for s in range(t)])\n",
    "        for i in range(T)])\n",
    "        \n",
    "        G2 = nb * sigma**2 / (p - nb - 1 + 1e-7)\n",
    "        \n",
    "        G3 = sum ([\n",
    "            sum([\n",
    "                1/(2*p) * (ns[s] * ns[ss]/ (p - nb - 1 + 1e-7)) * (ws[s] - ws[ss]).norm()**2\n",
    "            for s in range(t)])\n",
    "        for ss in range(t)])\n",
    "        \n",
    "        a = (1 - n/(p-(nb-n)))\n",
    "        \n",
    "        G5 = - (nb - n) * sigma**2 / (p - (nb - n) - 1 + 1e-7)\n",
    "        if t == 1:\n",
    "            G6 = 1/T * sum([w.norm()**2 for w in tasks])\n",
    "        else:\n",
    "            G6 = predict_theory_memreg(n,k,p,T,l,sigma,tasks, t = t - 1)\n",
    "        \n",
    "        G7 = -1/T * sum ([\n",
    "            sum([\n",
    "                ns[s]/p * (ws[s] - ws[i]).norm()**2\n",
    "            for s in range(t-1)])\n",
    "        for i in range(T)])\n",
    "        \n",
    "        G8 = -sum ([\n",
    "            sum([\n",
    "                1/(2*p) * (ns[s] * ns[ss]/ (p - (nb - n) - 1 + 1e-7)) * (ws[s] - ws[ss]).norm()**2\n",
    "            for s in range(t-1)])\n",
    "        for ss in range(t-1)])\n",
    "            \n",
    "        return G1 + G2 + G3 + a * (G5 + G6 + G7 + G8)\n",
    "            \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def predict_theory_mem(n, k, p, T, l, sigma, tasks):\n",
    "    ns = [k] * (T-1) + [n]\n",
    "    nb = sum(ns)\n",
    "    if nb + 1 <= p:\n",
    "        G1 = 1/T * (1 - nb/p) * sum([w.norm()**2 for w in tasks])\n",
    "        \n",
    "        G2 = sum ([sum([ \n",
    "            ns[t]/ (T*p) * (1 + (T/2*ns[i]) / (p-nb-1+1e-7)) * (tasks[t] - tasks[i]).norm()**2  \n",
    "            for t in range(T)]) for i in range(T)])\n",
    "        \n",
    "        G3 = nb * sigma**2 / (p - nb - 1 + 1e-7)\n",
    "        \n",
    "        return G1 + G2 + G3\n",
    "    elif p + 1 <= nb:\n",
    "        return 0\n",
    "\n",
    "def predict_loss_memreg(n, k, p, T, l, sigma, tasks, i, t):\n",
    "    # predicts the loss of the i-th task at time t for the memreg method\n",
    "    # E[||w_t - w^*_i||^2]\n",
    "    # assume t to be indexed from 1, i.e. t = 1, 2, ..., T\n",
    "    # assume i also to be indexed from 1, i.e. i = 1, 2, ..., T\n",
    "    if t < 0:\n",
    "        assert False\n",
    "        \n",
    "    ns = [k] * (t-1) + [n]\n",
    "    ws = tasks\n",
    "    nb = sum(ns)\n",
    "    if nb + 1 <= p:\n",
    "        G1 = sum([\n",
    "                ns[s]/p * (ws[s] - ws[i - 1]).norm()**2\n",
    "            for s in range(t)])\n",
    "        \n",
    "        G2 = nb * sigma**2 / (p - nb - 1 + 1e-7)\n",
    "        \n",
    "        G3 = sum ([\n",
    "            sum([\n",
    "                1/(2*p) * (ns[s] * ns[ss]/ (p - nb - 1 + 1e-7)) * (ws[s] - ws[ss]).norm()**2\n",
    "            for s in range(t)])\n",
    "        for ss in range(t)])\n",
    "        \n",
    "        a = (1 - n/(p-(nb-n)))\n",
    "        \n",
    "        G5 = - (nb - n) * sigma**2 / (p - (nb - n) - 1 + 1e-7)\n",
    "        if t == 1:\n",
    "            G6 = ws[i - 1].norm()**2\n",
    "        else:\n",
    "            G6 = predict_loss_memreg(n,k,p,T,l,sigma,tasks, i, t = t - 1)\n",
    "        \n",
    "        G7 = -sum([\n",
    "                ns[s]/p * (ws[s] - ws[i - 1]).norm()**2\n",
    "            for s in range(t-1)])\n",
    "        \n",
    "        \n",
    "        G8 = -sum ([\n",
    "            sum([\n",
    "                1/(2*p) * (ns[s] * ns[ss]/ (p - (nb - n) - 1 + 1e-7)) * (ws[s] - ws[ss]).norm()**2\n",
    "            for s in range(t-1)])\n",
    "        for ss in range(t-1)])\n",
    "            \n",
    "        return G1 + G2 + G3 + a * (G5 + G6 + G7 + G8)\n",
    "            \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def predict_loss_mem(n, k, p, T, l, sigma, tasks, i, t):\n",
    "    # predicts the loss of the i-th task at time t for the mem method\n",
    "    # E[||w_t - w^*_i||^2]\n",
    "    # assume t to be indexed from 1, i.e. t = 1, 2, ..., T\n",
    "    # assume i also to be indexed from 1, i.e. i = 1, 2, ..., T\n",
    "    \n",
    "    ns = [k] * (t-1) + [n]\n",
    "    ws = tasks\n",
    "    nb = sum(ns)\n",
    "    if nb + 1 <= p:\n",
    "        G1 = sum ([\n",
    "            ns[s]/p * (ws[s] - ws[i - 1]).norm()**2\n",
    "        for s in range(t)])\n",
    "        \n",
    "        G2 = nb * sigma**2 / (p - nb - 1 + 1e-7)\n",
    "        \n",
    "        G3 = sum ([\n",
    "            1/(2*p) * (ns[s] * ns[ss]/ (p - nb - 1 + 1e-7)) * (ws[s] - ws[ss]).norm()**2\n",
    "        for s in range(t) for ss in range(t)])\n",
    "        \n",
    "        G4 = (1 - nb / p) * (ws[i - 1]).norm()**2\n",
    "        \n",
    "        return G1 + G2 + G3 + G4\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def predict_forgetting_memreg(n, k, p, T, l, sigma, tasks):\n",
    "    f = 0\n",
    "    for i in range(1, T):\n",
    "        f += predict_loss_memreg(n, k, p, T, l, sigma, tasks, i, T) - predict_loss_memreg(n, k, p, T, l, sigma, tasks, i, i)\n",
    "    \n",
    "    return f / (T-1)\n",
    "\n",
    "def predict_forgetting_mem(n, k, p, T, l, sigma, tasks):\n",
    "    f = 0\n",
    "    for i in range(1, T):\n",
    "        f += predict_loss_mem(n, k, p, T, l, sigma, tasks, i, T) - predict_loss_mem(n, k, p, T, l, sigma, tasks, i, i)\n",
    "    \n",
    "    return f / (T-1)\n",
    "\n",
    "def predict_theory_sequential(n, p, T, l, sigma, tasks):\n",
    "    return predict_theory_mem(n, 0, p, T, l, sigma, tasks)\n",
    "\n",
    "def predict_theory_multi(n, p, T, l, sigma, tasks):\n",
    "    return predict_theory_mem(n, n, p, T, l, sigma, tasks)\n",
    "\n",
    "def predict_theory_single(n, p, T, l, sigma, tasks):\n",
    "    if n + 1 <= p:\n",
    "        G1 = 1/T * (1 - n/p) * sum([w.norm()**2 for w in tasks])\n",
    "\n",
    "        G3 = n * sigma**2 / (p - n - 1 + 1e-7)\n",
    "        assert G1 + G3 >= 0, f'G1 = {G1}, G3 = {G3}, n = {n}, p = {p}, T = {T}, sigma = {sigma}'\n",
    "        return G1 + G3\n",
    "    elif p + 1 <= n:\n",
    "        \n",
    "        G =  p * sigma**2 / (n - p - 1 + 1e-7)\n",
    "        assert G >= 0, f'G = {G}, n = {n}, p = {p}, T = {T}, sigma = {sigma}'\n",
    "        return G\n",
    "\n",
    "\n",
    "def evaluate_sequential(n, k, p, T, l, sigma, method, tasks):\n",
    "\n",
    "    X_1T, Y_1T = get_X_and_Y(n, p, T, sigma, tasks)\n",
    "    if method == \"sequential\":\n",
    "        ws = solve_sequential(n, p, T, l, X_1T, Y_1T)\n",
    "    elif method == \"continual\":\n",
    "        ws = solve_continual(n, p, T, l, X_1T, Y_1T)\n",
    "    elif method == \"memreg\":\n",
    "        ws = solve_memreg(n, k, p, T, l, X_1T, Y_1T)\n",
    "    elif method == \"mem\":\n",
    "        ws = solve_mem(n, k, p, T, l, X_1T, Y_1T)\n",
    "    else:\n",
    "        raise ValueError(\"invalid method\")\n",
    "\n",
    "    w = ws[-1]\n",
    "\n",
    "    average_error = 0\n",
    "    for i in range(T):\n",
    "        w_star = tasks[i]\n",
    "        average_error += get_loss(w,w_star)\n",
    "    average_error /= T\n",
    "    \n",
    "    average_forgetting = 0\n",
    "    for i in range(T-1):\n",
    "        w_star = tasks[i]\n",
    "        average_forgetting += get_loss(ws[-1],w_star) - get_loss(ws[i],w_star)\n",
    "    \n",
    "    average_forgetting /= (T-1)\n",
    "        \n",
    "    return average_error, average_forgetting\n",
    "\n",
    "def evaluate_multiple_sequentials(n, k, p, T, l, sigma, method, tasks, num_trials):\n",
    "    average_errors = []\n",
    "    average_forgettings = []\n",
    "    \n",
    "    for _ in range(num_trials):\n",
    "        error, forgetting = evaluate_sequential(n, k, p, T, l, sigma, method, tasks)\n",
    "        average_errors.append(error)\n",
    "        average_forgettings.append(forgetting)\n",
    "        \n",
    "    return np.mean (average_errors), np.mean (average_forgettings)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paper Plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fig1 - Multi-task vs Single-task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import adjust_plots\n",
    "adjust_plots(font_scale=1.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "T = 10\n",
    "ps = (list(range(10, 50, 1)) + list(range(51, 100, 1)) +\n",
    "      list(range(100, 450, 10)) + \n",
    "      list(range(450, 500, 5)) +\n",
    "      list(range(501, 550, 5)) + list(range(550, 1000, 10)) + list(range(1000, 10000, 100)) + [50000])\n",
    "ps_ = np.int32 (10**np.linspace(np.log10(20), np.log10(10000), 10)) #list(range(20, 100, 10)) + list(range(100, 1000, 100)) + list(range(1000, 10000, 1000)) # Practical Ps\n",
    "n = 50\n",
    "\n",
    "\n",
    "theta = 7 * math.pi / 8\n",
    "norms = 1\n",
    "delta = 0.\n",
    "l = 0.\n",
    "\n",
    "s = 10\n",
    "u = 0\n",
    "true_tasks = get_true_tasks(T, s, u, theta, norms)\n",
    "print (true_tasks[0].shape)\n",
    "\n",
    "\n",
    "sigmas = [0, 0.3, 1]     \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_values_theory_multi = {}\n",
    "plot_values_practical_multi = {}\n",
    "plot_values_theory_single = {}\n",
    "plot_values_practical_single = {}\n",
    "\n",
    "\n",
    "for sigma in sigmas:\n",
    "    plot_values_theory_multi[sigma] = []\n",
    "    plot_values_practical_multi[sigma] = []\n",
    "    plot_values_theory_single[sigma] = []\n",
    "    plot_values_practical_single[sigma] = []\n",
    "    \n",
    "    for p in tqdm(ps):\n",
    "        num_trials = 500 if p < 1000 else 50\n",
    "        \n",
    "        tasks = get_tasks(T, p, true_tasks, theta, norms, selection_method='random_crop')\n",
    "\n",
    "        plot_values_theory_single[sigma].append(predict_theory_single(n, p, T, l, sigma, tasks))\n",
    "        \n",
    "        if p >= T * n + 2:\n",
    "            plot_values_theory_multi[sigma].append(predict_theory_multi(n, p, T, l, sigma, tasks))\n",
    "        \n",
    "        plot_values_practical_multi[sigma].append(evaluate_multiple_multi_tasks(n, p, T, l, sigma, tasks, num_trials))\n",
    "        \n",
    "        plot_values_practical_single[sigma].append(evaluate_multiple_single_tasks(n, p, T, l, sigma, tasks, num_trials))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ps = np.array(ps)\n",
    "\n",
    "for sigma in sigmas:\n",
    "    plot_values_theory_multi[sigma] = np.array(plot_values_theory_multi[sigma])\n",
    "    plot_values_practical_multi[sigma] = np.array(plot_values_practical_multi[sigma])\n",
    "\n",
    "    plot_values_theory_single[sigma] = np.array(plot_values_theory_single[sigma])\n",
    "    plot_values_practical_single[sigma] = np.array(plot_values_practical_single[sigma])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def apply_interpolation(x, y, count = 100):\n",
    "    x_new = np.linspace(x.min(), x.max(), count)\n",
    "    y_new = np.interp(x_new, x, y)\n",
    "    return x_new, y_new "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['red', 'green', 'blue', 'purple', 'orange', 'gray']\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "filter = (ps >= (T * n + 2))\n",
    "\n",
    "\n",
    "\n",
    "for i, sigma in enumerate (sigmas):\n",
    "\n",
    "    color = colors[i]\n",
    "    plt.plot(np.log10 (ps)[filter], plot_values_theory_multi[sigma], label=f\"multi-task, $\\\\sigma$ = {sigma}\", color=color)\n",
    "    plt.plot(np.log10 (ps), plot_values_practical_multi[sigma], linestyle=':', color=color, linewidth=2.5)\n",
    "\n",
    "for i, sigma in enumerate (sigmas):\n",
    "    color = colors[i+3]\n",
    "    plt.plot(np.log10 (ps), plot_values_theory_single[sigma], label=f\"single-task, $\\\\sigma$ = {sigma}\", color=color)\n",
    "    plt.plot(np.log10 (ps), plot_values_practical_single[sigma], linestyle=':', color=color, linewidth=2.5)\n",
    "\n",
    "plt.xticks([1, np.log10(50), 2, np.log10(500), 3, 4], [10,50, 100, 500, 1000,10000])\n",
    "\n",
    "\n",
    "plt.ylim(-0.5, 5.5)\n",
    "plt.xlabel(\"p\")\n",
    "plt.xlim(1, 4.2)\n",
    "# if theta/math.pi < 0.5:\n",
    "\n",
    "if theta/math.pi < 0.5:\n",
    "    plt.ylabel(\"Average Generalization Error ($G$)\")\n",
    "    plt.legend()\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# set the legend to be top right\n",
    "# plt.legend(loc='upper right')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(f'Figs/multivssinge_theta_{theta/math.pi:.2f}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot multi-task - single task in another plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, sigma in enumerate (sigmas):\n",
    "    color = colors[i]\n",
    "    plt.plot(np.log10 (ps)[filter], plot_values_theory_single[sigma][filter] - plot_values_theory_multi[sigma], label=f\"$\\\\sigma$ = {sigma}\", color=color)\n",
    "    plt.plot(np.log10 (ps), plot_values_practical_single[sigma] - plot_values_practical_multi[sigma], linestyle=\"--\", color=color)\n",
    "\n",
    "plt.xticks([1, np.log10(50), 2, np.log10(500), 3, 4], [10,50, 100, 500, 1000,10000])\n",
    "\n",
    "\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.xlabel(\"$p$\")\n",
    "plt.xlim(1, 4.2)\n",
    "# if theta/math.pi < 0.5:\n",
    "\n",
    "plt.ylabel(\"Average Knowledge Transfer ($K$)\")\n",
    "if theta/math.pi < 0.5:\n",
    "    plt.legend()\n",
    "else:\n",
    "    pass\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "# plt.savefig(f'Figs/kt_multivssinge_theta_{theta/math.pi:.2f}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# save_object = [ps, plot_values_theory_multi, plot_values_practical_multi, plot_values_theory_single, plot_values_practical_single]\n",
    "# torch.save(save_object, f'Figs/saves/multivssinge_theta_{theta/math.pi:.2f}.pt')\n",
    "\n",
    "# ps, plot_values_theory_multi, plot_values_practical_multi, plot_values_theory_single, plot_values_practical_single = torch.load(f'Figs/saves/multivssinge_theta_{theta/math.pi:.2f}.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fig1 - Mem CL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "T = 10\n",
    "ps = (list(range(10, 50, 1)) + list(range(51, 100, 1)) +\n",
    "      list(range(100, 130, 5)) +\n",
    "      list(range(130, 150, 2)) +\n",
    "      list(range(150, 210, 5)) +\n",
    "      list(range(210, 250, 2)) +\n",
    "      list(range(250, 300, 10)) +\n",
    "      list(range(300, 340, 2)) +\n",
    "      list(range(340, 390, 10)) +\n",
    "      list(range(390, 420, 5)) +\n",
    "      list(range(420, 490, 10)) +\n",
    "      list(range(490, 510, 5)) +\n",
    "      list(range(450, 550, 5)) +\n",
    "      list(range(550, 1000, 10)) + list(range(1000, 10000, 100)) + [50000])\n",
    "\n",
    "ps = sorted(ps)\n",
    " \n",
    "      \n",
    "ps_ = np.int32 (10**np.linspace(np.log10(20), np.log10(10000), 10)) #list(range(20, 100, 10)) + list(range(100, 1000, 100)) + list(range(1000, 10000, 1000)) # Practical Ps\n",
    "n = 50\n",
    "\n",
    "\n",
    "theta = math.pi / 8\n",
    "norms = 1\n",
    "sigma = 0.\n",
    "l = 0.\n",
    "\n",
    "s = 10\n",
    "u = 0\n",
    "true_tasks = get_true_tasks(T, s, u, theta, norms)\n",
    "print (true_tasks[0].shape)\n",
    "\n",
    "deltas = [0, 0.2, 0.4, 0.6, 0.8, 1.]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_values_theory_mem = {}\n",
    "plot_values_practical_mem = {}\n",
    "\n",
    "plot_values_theory_mem_forgetting = {}\n",
    "plot_values_practical_mem_forgetting = {}\n",
    "\n",
    "\n",
    "for delta in deltas:\n",
    "    k = int(delta * n)\n",
    "    plot_values_theory_mem[delta] = []\n",
    "    plot_values_practical_mem[delta] = []\n",
    "    \n",
    "    plot_values_theory_mem_forgetting[delta] = []\n",
    "    plot_values_practical_mem_forgetting[delta] = []\n",
    "    \n",
    "    for p in tqdm(ps):\n",
    "        tasks = get_tasks(T, p, true_tasks, theta, norms, selection_method='random_crop')\n",
    "        num_trials = 500 if p < 1000 else 50\n",
    "        \n",
    "        if p >= (T - 1) * k + n + 1:\n",
    "            plot_values_theory_mem[delta].append(predict_theory_mem(n, k, p, T, l, sigma, tasks))\n",
    "            plot_values_theory_mem_forgetting[delta].append(predict_forgetting_mem(n, k, p, T, l, sigma, tasks))\n",
    "            \n",
    "        tasks = get_tasks(T, p, true_tasks, theta, norms, selection_method='random_crop')\n",
    "        e, f = evaluate_multiple_sequentials(n, k, p, T, l, sigma, \"mem\", tasks, num_trials)\n",
    "        plot_values_practical_mem[delta].append(e)\n",
    "        plot_values_practical_mem_forgetting[delta].append(f)\n",
    "        \n",
    "           \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ps = np.array(ps)\n",
    "\n",
    "for delta in deltas:\n",
    "    plot_values_practical_mem[delta] = np.array(plot_values_practical_mem[delta])\n",
    "    plot_values_theory_mem[delta] = np.array(plot_values_theory_mem[delta])\n",
    "    \n",
    "    plot_values_practical_mem_forgetting[delta] = np.array(plot_values_practical_mem_forgetting[delta])\n",
    "    plot_values_theory_mem_forgetting[delta] = np.array(plot_values_theory_mem_forgetting[delta])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['red', 'green', 'blue', 'gray', 'orange', 'purple']\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i, delta in enumerate (deltas):\n",
    "    x_new, y_new = apply_interpolation(np.log10 (ps), plot_values_practical_mem[delta], count = 200)\n",
    "    \n",
    "    color = colors[i]\n",
    "    label = rf'$m$ = {int(delta * n)}'    \n",
    "    if delta == 0.0:\n",
    "        label += ' (sequential)'\n",
    "    elif delta == 1.0:\n",
    "        label += ' (multi-task)'        \n",
    "    \n",
    "    k = int(delta * n)\n",
    "    filter = (ps >= ((T - 1) * k + n + 1))\n",
    "    \n",
    "    plt.plot(np.log10 (ps)[filter], plot_values_theory_mem[delta], label = label, color=color)\n",
    "    plt.plot(np.log10 (ps), plot_values_practical_mem[delta], color=color, linestyle='dotted', linewidth=2.5)\n",
    "    # plt.plot(x_new, y_new, color=color, marker = 'o', markersize=3.5, linestyle = ' ')\n",
    "\n",
    "plt.xticks([1, np.log10(50), 2, np.log10(500), 3, 4], [10,50, 100, 500, 1000,10000])\n",
    "\n",
    "plt.xlim(1, 4.2)\n",
    "plt.xlabel(\"p\")\n",
    "\n",
    "\n",
    "# set the legend to be top right\n",
    "# plt.legend(loc='lower right')\n",
    "\n",
    "\n",
    "\n",
    "if theta/math.pi < 0.5:\n",
    "    plt.ylabel(\"Average Generalization Error ($G$)\")\n",
    "    plt.legend()\n",
    "    plt.ylim(-0.1, 1.05)\n",
    "else:\n",
    "    plt.ylim(0.8, 4.05)\n",
    "    \n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(f'Figs/mem_method_{theta/math.pi:.2f}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "is_sorted = lambda a: np.all(a[:-1] <= a[1:])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "is_sorted(ps)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "plot_values_theory_memreg = {}\n",
    "plot_values_practical_memreg = {}\n",
    "\n",
    "plot_values_theory_memreg_forgetting = {}\n",
    "plot_values_practical_memreg_forgetting = {}\n",
    "\n",
    "deltas = [0, 0.2, 0.4, 0.6, 0.8, 1.]\n",
    "for delta in deltas:\n",
    "    k = int(delta * n)\n",
    "    plot_values_theory_memreg[delta] = []\n",
    "    plot_values_practical_memreg[delta] = []\n",
    "    \n",
    "    plot_values_theory_memreg_forgetting[delta] = []\n",
    "    plot_values_practical_memreg_forgetting[delta] = []\n",
    "    \n",
    "    for p in tqdm(ps):\n",
    "        tasks = get_tasks(T, p, true_tasks, theta, norms, selection_method='random_crop')\n",
    "        \n",
    "        if p >= (T - 1) * k + n + 1:\n",
    "            plot_values_theory_memreg[delta].append(predict_theory_memreg(n, k, p, T, l, sigma, tasks))\n",
    "            plot_values_theory_memreg_forgetting[delta].append(predict_forgetting_memreg(n, k, p, T, l, sigma, tasks))\n",
    "        \n",
    "    \n",
    "        num_trials = 500 if p < 1000 else 50\n",
    "    \n",
    "        e, f = evaluate_multiple_sequentials(n, k, p, T, l, sigma, \"memreg\", tasks, num_trials)\n",
    "        plot_values_practical_memreg[delta].append(e)\n",
    "        plot_values_practical_memreg_forgetting[delta].append(f)\n",
    "        \n",
    "        \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ps = np.array(ps)\n",
    "\n",
    "for delta in deltas:\n",
    "    plot_values_practical_memreg[delta] = np.array(plot_values_practical_memreg[delta])\n",
    "    plot_values_theory_memreg[delta] = np.array(plot_values_theory_memreg[delta])\n",
    "    \n",
    "    plot_values_practical_memreg_forgetting[delta] = np.array(plot_values_practical_memreg_forgetting[delta])\n",
    "    plot_values_theory_memreg_forgetting[delta] = np.array(plot_values_theory_memreg_forgetting[delta])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['red', 'green', 'blue', 'gray', 'orange', 'purple']\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i, delta in enumerate (deltas):\n",
    "    color = colors[i]\n",
    "    label = rf'$m$ = {int(delta * n)}'    \n",
    "    if delta == 0.0:\n",
    "        label += ' (sequential)'\n",
    "    elif delta == 1.0:\n",
    "        label += ' (multi-task)'        \n",
    "    \n",
    "    k = int(delta * n)\n",
    "    filter = (ps >= ((T - 1) * k + n + 1))\n",
    "    \n",
    "    plt.plot(np.log10 (ps)[filter], plot_values_theory_memreg[delta], label = label, color=color)\n",
    "    plt.plot(np.log10 (ps), plot_values_practical_memreg[delta], color=color, linestyle='dotted', linewidth=2.5)\n",
    "\n",
    "plt.xticks([1, np.log10(50), 2, np.log10(500), 3, 4], [10,50, 100, 500, 1000,10000])\n",
    "\n",
    "plt.xlim(1, 4.1)\n",
    "plt.xlabel(\"$p$\")\n",
    "\n",
    "plt.ylabel(\"Average Generalization Error ($G$)\")\n",
    "\n",
    "\n",
    "if theta/math.pi < 0.5:\n",
    "    plt.legend()\n",
    "    plt.ylim(-0.1, 1.05)\n",
    "else:\n",
    "    plt.ylim(0.8, 4.05)\n",
    "\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(f'Figs/memreg_method_{theta/math.pi:.2f}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot_values_practical_mem_forgetting, plot_values_theory_memreg_forgetting, plot_values_practical_memreg_forgetting]\n",
    "# \n",
    "# torch.save(save_object, f'Figs/saves/mem_methods_{theta/math.pi:.2f}.pt')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# ps, plot_values_theory_mem, plot_values_practical_mem, plot_values_theory_memreg, plot_values_practical_memreg, plot_values_theory_mem_forgetting, plot_values_practical_mem_forgetting, plot_values_theory_memreg_forgetting, plot_values_practical_memreg_forgetting = torch.load(f'Figs/saves/mem_methods_{theta/math.pi:.2f}.pt')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
